<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZKXM8P5V7J"></script> <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZKXM8P5V7J');
</script> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Khai Nguyen </title> <meta name="author" content="Khai Nguyen"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="/assets/libs/mdb/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="/assets/libs/google_fonts/google-fonts.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/khai2.png?af050666375dfb779f4266b6022dfc7b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://khainb.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6B%68%61%69%6E%62@%75%74%65%78%61%73.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/khainb" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/khai-nguyen-307895155" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://www.researchgate.net/profile/Khai-Nguyen-37/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://scholar.google.com/citations?user=im5fNaQAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/khainb_ml" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Khai</span> Nguyen </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/khai2-480.webp 480w,/assets/img/khai2-800.webp 800w,/assets/img/khai2-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/khai2.png?af050666375dfb779f4266b6022dfc7b" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="khai2.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>Hi! I’m Khai, a fourth-year Ph.D. candidate at <a href="https://stat.utexas.edu/" rel="external nofollow noopener" target="_blank">Department of Statistics and Data Sciences</a>, <a href="https://www.utexas.edu/" rel="external nofollow noopener" target="_blank">University of Texas at Austin</a>. I am fortunate to be advised by Professor <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> and Professor <a href="https://www.ma.utexas.edu/component/cobalt/item/15-mathematics/364-mueller-peter?Itemid=1259" rel="external nofollow noopener" target="_blank">Peter Müller</a>, and to be associated with Institute for Foundations of Machine Learning (<a href="https://www.ifml.institute/" rel="external nofollow noopener" target="_blank">IFML</a>). I graduated from <a href="https://soict.hust.edu.vn/" rel="external nofollow noopener" target="_blank">Hanoi University of Science and Technology</a> with a Computer Science Bachelor’s degree. Before joining UT Austin, I was an AI Research Resident at <a href="http://www.vinai.io" rel="external nofollow noopener" target="_blank">VinAI Research</a> under the supervision of <a href="https://sites.google.com/site/buihhung/" rel="external nofollow noopener" target="_blank">Dr. Hung Bui</a>.</p> <p>I’m always open to collaborations, discussions, and exploring new opportunities. Don’t hesitate to reach out if you’re interested in my research or want to discuss potential research projects.</p> <p><img src="https://khainb.github.io/assets/intro.gif" alt="GIF description" style="width: 100%; height: auto;"></p> <p><em>(This video is created by my proposed <a href="https://arxiv.org/pdf/2304.13586.pdf" rel="external nofollow noopener" target="_blank">energy-based sliced Wasserstein distance</a>.)</em></p> <p><strong>Research:</strong> My research focuses on both fundamental problems and applied problems in probabilistic machine learning, deep learning, and statistics.</p> <p><strong><em>1. Computational Optimal Transport.</em></strong> My research makes <a href="https://en.wikipedia.org/wiki/Transportation_theory_(mathematics)" rel="external nofollow noopener" target="_blank">Optimal Transport</a> scalable in statistical inference (low time complexity, low space complexity, low sample complexity) via the one-dimensional projection approach which is known as sliced optimal transport (sliced Wasserstein distance). My work focuses on three key aspects of sliced Wasserstein: numerical approximation, projecting operator, and slicing distribution.</p> <p><strong><em>2. Efficiency, Scalability, Interpretability, and Trustworthiness of Machine Learning.</em></strong> My research enhances the performance of 3D vision models, speeds up the training of generative models, adapts prediction models to new unseen domains, explains multimodal transferable representation, and ensures fairness and robustness in learning processes.</p> <p><strong><em>3. Machine Learning for Biological Sciences.</em></strong> My research quantifies interactions between cell types which drive various physiological and pathological processes.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Mar 07, 2025</th> <td> My proposal <strong>Summarizing Bayesian Nonparametric Mixture Posterior - Sliced Optimal Transport Metrics for Gaussian Mixtures</strong> is accepted at <strong>The Bayesian Young Statisticians Meeting 2025</strong> as a talk in a session with discussion. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 27, 2025</th> <td> I’m thrilled to be awarded a travel grant for International Conference on Bayesian Nonparametrics (BNP 14). </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 26, 2025</th> <td> I’m thrilled to be awarded a <a href="https://gradschool.utexas.edu/funding/fellowships/grad-school/continuing" rel="external nofollow noopener" target="_blank"><strong>UT Austin Continuing Fellowship</strong></a> which is a merit-based fellowship awarded based on academic achievements, research accomplishments, and potential for future contributions. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 11, 2025</th> <td> Our paper <strong>Towards Marginal Fairness Sliced Wasserstein Barycenter</strong> is selected as a <strong>spotlight</strong> at <strong>ICLR 2025</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 26, 2025</th> <td> My proposal <strong>Summarizing Bayesian Nonparametric Mixture Posterior - Sliced Optimal Transport Metrics for Gaussian Mixtures</strong> is accepted at <strong>14th International Conference on Bayesian Nonparametrics</strong> as a contributed talk. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 22, 2025</th> <td> 1 paper <strong>Towards Marginal Fairness Sliced Wasserstein Barycenter</strong> is accepted at <strong>ICLR 2025</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 26, 2024</th> <td> 1 paper <strong>Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions</strong> is accepted at <strong>NeurIPS 2024</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">May 01, 2024</th> <td> 1 paper <strong>Sliced Wasserstein with Random-Path Projecting Directions</strong> is accepted at <strong>ICML 2024</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 27, 2024</th> <td> 1 paper <strong>Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning</strong> is accepted at <strong>CVPR 2024</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 19, 2024</th> <td> 2 papers <strong>Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts</strong>, <strong>On Parameter Estimation in Deviated Gaussian Mixture of Experts</strong> are accepted at <strong>AISTATS 2024</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 16, 2024</th> <td> 4 papers <strong>Quasi-Monte Carlo for 3D Sliced Wasserstein - Spotlight Presentation</strong>, <strong>Sliced Wasserstein Estimation with Control Variates</strong>, <strong>Diffeomorphic Deformation via Sliced Wasserstein Distance Optimization for Cortical Surface Reconstruction</strong>, and <strong>Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation</strong> are accepted at <strong>ICLR 2024</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 21, 2023</th> <td> 4 papers <strong>Energy-Based Sliced Wasserstein Distance</strong>, <strong>Markovian sliced Wasserstein distances: Beyond independent projections</strong>, <strong>Designing robust Transformers using robust kernel density estimation</strong>, and <strong>Minimax optimal rate for parameter estimation in multivariate deviated models</strong> are accepted at <strong>NeurIPS 2023</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 24, 2023</th> <td> 1 paper <strong>Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction</strong> is accepted at <strong>ICML 2023</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 20, 2023</th> <td> 1 paper <strong>Hierarchical Sliced Wasserstein Distance</strong> is accepted at <strong>ICLR 2023</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 14, 2022</th> <td> 4 papers <strong>Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution</strong>, <strong>Amortized Projection Optimization for Sliced Wasserstein Generative Models</strong>, <strong>Improving Transformer with an Admixture of Attention Heads</strong> , and <strong>FourierFormer: Transformer Meets Generalized Fourier Integral Theorem</strong> are accepted at <strong>NeurIPS 2022</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 24, 2022</th> <td> 2 papers <strong>Improving Mini-batch Optimal Transport via Partial Transportation</strong> and <strong>On Transportation of Mini-batches: A Hierarchical Approach</strong> are accepted at <strong>ICML 2022</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 24, 2021</th> <td> 2 papers <strong>Distributional Sliced-Wasserstein and Applications to Generative Modeling - Spotlight Presentation</strong> and <strong>Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein</strong> are accepted at <strong>ICLR 2021</strong>. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#820000"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> <span class="award badge rounded w-100"> Spotlight </span> </div> <div id="nguyen2025fair" class="col-sm-8"> <div class="title">Towards Marginal Fairness Sliced Wasserstein Barycenter</div> <div class="author"> <em>Khai Nguyen<sup>*</sup></em> , Hai Nguyen<sup>*</sup>, and <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> </div> <div class="periodical"> <em>International Conference on Learning Representations</em>, 2025 </div> <div class="periodical"> </div> <span class="honor"> Spotlight Presentation [3.2%] </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2405.07482.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/khainb/MFSWB" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:ldfaerwXgEUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Spotlight</p> </div> <div class="abstract hidden"> <p>The sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization. As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem. Due to the computational disadvantages of the formal definition, we propose two hyperparameter-free and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time. To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions. We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance. Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0048BA"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="nguyen2024hybrid" class="col-sm-8"> <div class="title">Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions</div> <div class="author"> <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> </div> <div class="periodical"> <em>Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2404.15378.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/khainb/H2SW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:2P1L_qKh6hAC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been widely used in applications due to their computational and statistical scalability. However, the SW and the GSW are only defined between distributions supported on a homogeneous domain. This limitation prevents their usage in applications with heterogeneous joint distributions with marginal distributions supported on multiple different domains. Using SW and GSW directly on the joint domains cannot make a meaningful comparison since their homogeneous slicing operator i.e., Radon Transform (RT) and Generalized Radon Transform (GRT) are not expressive enough to capture the structure of the joint supports set. To address the issue, we propose two new slicing operators i.e., Partial Generalized Radon Transform (PGRT) and Hierarchical Hybrid Radon Transform (HHRT). In greater detail, PGRT is the generalization of Partial Radon Transform (PRT), which transforms a subset of function arguments non-linearly while HHRT is the composition of PRT and multiple domain-specific PGRT on marginal domain arguments. By using HHRT, we extend the SW into Hierarchical Hybrid Sliced Wasserstein (H2SW) distance which is designed specifically for comparing heterogeneous joint distributions. We then discuss the topological, statistical, and computational properties of H2SW. Finally, we demonstrate the favorable performance of H2SW in 3D mesh deformation, deep 3D mesh autoencoders, and datasets comparison.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#4E6C50"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> </div> <div id="nguyen2024sliced" class="col-sm-8"> <div class="title">Sliced Wasserstein with Random-Path Projecting Directions</div> <div class="author"> <em>Khai Nguyen</em>, Shujian Zhang , Tam Le, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Nhat Ho' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>International Conference on Machine Learning</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2401.15889.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/khainb/RPSW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:L8Ckcad2t8MC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-8-4285F4?logo=googlescholar&amp;labelColor=beige" alt="8 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Slicing distribution selection has been used as an effective technique to improve the performance of parameter estimators based on minimizing sliced Wasserstein distance in applications. Previous works either utilize expensive optimization to select the slicing distribution or use slicing distributions that require expensive sampling methods. In this work, we propose an optimization-free slicing distribution that provides a fast sampling for the Monte Carlo estimation of expectation. In particular, we introduce the random-path projecting direction (RPD) which is constructed by leveraging the normalized difference between two random vectors following the two input measures. From the RPD, we derive the random-path slicing distribution (RPSD) and two variants of sliced Wasserstein, i.e., the Random-Path Projection Sliced Wasserstein (RPSW) and the Importance Weighted Random-Path Projection Sliced Wasserstein (IWRPSW). We then discuss the topological, statistical, and computational properties of RPSW and IWRPSW. Finally, we showcase the favorable performance of RPSW and IWRPSW in gradient flow and the training of denoising diffusion generative models on images.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#820000"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> <span class="award badge rounded w-100"> Spotlight </span> </div> <div id="nguyen2023quasi" class="col-sm-8"> <div class="title">Quasi-Monte Carlo for 3D Sliced Wasserstein</div> <div class="author"> <em>Khai Nguyen</em>, <a href="https://nbariletto.github.io/" rel="external nofollow noopener" target="_blank">Nicolas Bariletto</a>, and <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> </div> <div class="periodical"> <em>International Conference on Learning Representations</em>, 2024 </div> <div class="periodical"> </div> <span class="honor"> Spotlight Presentation [5%] </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2309.11713.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/khainb/Quasi-SW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:7PzlFSSx8tAC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-16-4285F4?logo=googlescholar&amp;labelColor=beige" alt="16 Google Scholar citations"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Spotlight</p> </div> <div class="abstract hidden"> <p>Monte Carlo (MC) approximation has been used as the standard computation approach for the Sliced Wasserstein (SW) distance, which has an intractable expectation in its analytical form. However, the MC method is not optimal in terms of minimizing the absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically verify various ways of constructing QMC points sets on the 3D unit-hypersphere, including Gaussian-based mapping, equal area mapping, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimation for stochastic optimization, we extend QSW into Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed low-discrepancy sequences. For theoretical properties, we prove the asymptotic convergence of QSW and the unbiasedness of RQSW. Finally, we conduct experiments on various 3D tasks, such as point-cloud comparison, point-cloud interpolation, image style transfer, and training deep point-cloud autoencoders, to demonstrate the favorable performance of the proposed QSW and RQSW variants.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#820000"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="nguyen2023control" class="col-sm-8"> <div class="title">Sliced Wasserstein Estimation with Control Variates</div> <div class="author"> <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> </div> <div class="periodical"> <em>International Conference on Learning Representations</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2305.00402.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/khainb/CV-SW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:Wp0gIr-vW9MC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-16-4285F4?logo=googlescholar&amp;labelColor=beige" alt="16 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The sliced Wasserstein (SW) distances between two probability measures are defined as the expectation of the Wasserstein distance between two one-dimensional projections of the two measures. The randomness comes from a projecting direction that is used to project the two input measures to one dimension. Due to the intractability of the expectation, Monte Carlo integration is performed to estimate the value of the SW distance. Despite having various variants, there has been no prior work that improves the Monte Carlo estimation scheme for the SW distance in terms of controlling its variance. To bridge the literature on variance reduction and the literature on the SW distance, we propose computationally efficient control variates to reduce the variance of the empirical estimation of the SW distance. The key idea is to first find Gaussian approximations of projected one-dimensional measures, then we utilize the closed-form of the Wasserstein-2 distance between two Gaussian distributions to design the control variates. In particular, we propose using a lower bound and an upper bound of the Wasserstein-2 distance between two fitted Gaussians as two computationally efficient control variates. We empirically show that the proposed control variate estimators can help to reduce the variance considerably when comparing measures over images and point-clouds. Finally, we demonstrate the favorable performance of the proposed control variate estimators in gradient flows to interpolate between two point-clouds and in deep generative modeling on standard image datasets, such as CIFAR10 and CelebA.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0048BA"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="nguyen2023energy" class="col-sm-8"> <div class="title">Energy-Based Sliced Wasserstein Distance</div> <div class="author"> <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> </div> <div class="periodical"> <em>Neural Information Processing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2304.13586.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/khainb/EBSW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:4DMP91E08xMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-27-4285F4?logo=googlescholar&amp;labelColor=beige" alt="27 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The sliced Wasserstein (SW) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. A key component of the SW distance is the slicing distribution. There are two existing approaches for choosing this distribution. The first approach is using a fixed prior distribution. The second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. However, both approaches have their limitations. A fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. Doing optimization for the best distribution is often expensive and unstable. Moreover, designing the parametric family of the candidate distribution could be easily misspecified. To address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter-free and has the density proportional to an energy function of the projected one-dimensional Wasserstein distance. We then derive a novel sliced Wasserstein metric, energy-based sliced Waserstein (EBSW) distance, and investigate its topological, statistical, and computational properties via importance sampling, sampling importance resampling, and Markov Chain methods. Finally, we conduct experiments on point-cloud gradient flow, color transfer, and point-cloud reconstruction to show the favorable performance of the EBSW.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0048BA"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="nguyen2023markov" class="col-sm-8"> <div class="title">Markovian Sliced Wasserstein Distances: Beyond Independent Projections</div> <div class="author"> <em>Khai Nguyen</em>, <a href="https://www.cs.utexas.edu/~tzren/" rel="external nofollow noopener" target="_blank">Tongzheng Ren</a>, and <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> </div> <div class="periodical"> <em>Neural Information Processing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2301.03749.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/UT-Austin-Data-Science-Group/MSW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:M3ejUd6NZC8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-8-4285F4?logo=googlescholar&amp;labelColor=beige" alt="8 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Sliced Wasserstein (SW) distance suffers from redundant projections due to independent uniform random projecting directions. To partially overcome the issue, max K sliced Wasserstein (Max-K-SW) distance (K≥1), seeks the best discriminative orthogonal projecting directions. Despite being able to reduce the number of projections, the metricity of Max-K-SW cannot be guaranteed in practice due to the non-optimality of the optimization. Moreover, the orthogonality constraint is also computationally expensive and might not be effective. To address the problem, we introduce a new family of SW distances, named Markovian sliced Wasserstein (MSW) distance, which imposes a first-order Markov structure on projecting directions. We discuss various members of MSW by specifying the Markov structure including the prior distribution, the transition distribution, and the burning and thinning technique. Moreover, we investigate the theoretical properties of MSW including topological properties (metricity, weak convergence, and connection to other distances), statistical properties (sample complexity, and Monte Carlo estimation error), and computational properties (computational complexity and memory complexity). Finally, we compare MSW distances with previous SW variants in various applications such as gradient flows, color transfer, and deep generative modeling to demonstrate the favorable performance of MSW.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#4E6C50"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> </div> <div id="nguyen2023selfattention" class="col-sm-8"> <div class="title">Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction</div> <div class="author"> <em>Khai Nguyen<sup>*</sup></em>, <a href="https://hsgser.github.io/" rel="external nofollow noopener" target="_blank">Dang Nguyen<sup>*</sup></a>, and <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> </div> <div class="periodical"> <em>International Conference on Machine Learning</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2301.04791.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/hsgser/Self-Amortized-DSW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:qxL8FJ1GzNcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-10-4285F4?logo=googlescholar&amp;labelColor=beige" alt="10 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Max sliced Wasserstein (Max-SW) distance has been widely known as a solution for redundant projections of sliced Wasserstein (SW) distance. In applications that have various independent pairs of probability measures, amortized projection optimization is utilized to predict the “max" projecting directions given two input measures instead of using projected gradient ascent multiple times. Despite being efficient, the first issue of the current framework is the violation of permutation invariance property and symmetry property. To address the issue, we propose to design amortized models based on self-attention architecture. Moreover, we adopt efficient self-attention architectures to make the computation linear in the number of supports. Secondly, Max-SW and its amortized version cannot guarantee metricity property due to the sub-optimality of the projected gradient ascent and the amortization gap. Therefore, we propose to replace Max-SW with distributional sliced Wasserstein distance with von Mises-Fisher (vMF) projecting distribution (v-DSW). Since v-DSW is a metric with any non-degenerate vMF distribution, its amortized version can guarantee the metricity when predicting the best discriminate projecting distribution. With the two improvements, we derive self-attention amortized distributional projection optimization and show its appealing performance in point-cloud reconstruction and its downstream applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#820000"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="nguyen2022hsw" class="col-sm-8"> <div class="title">Hierarchical Sliced Wasserstein Distance</div> <div class="author"> <em>Khai Nguyen</em>, <a href="https://www.cs.utexas.edu/~tzren/" rel="external nofollow noopener" target="_blank">Tongzheng Ren</a> , <a href="https://huynm99.github.io/" rel="external nofollow noopener" target="_blank">Huy Nguyen</a>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Litu Rout, Tan Nguyen, Nhat Ho' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>International Conference on Learning Representations</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2209.13570.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/UT-Austin-Data-Science-Group/HSW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:KlAtU1dfN6UC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-29-4285F4?logo=googlescholar&amp;labelColor=beige" alt="29 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Sliced Wasserstein (SW) distance has been widely used in different application scenarios since it can be scaled to a large number of supports without suffering from the curse of dimensionality. The value of sliced Wasserstein distance is the average of transportation cost between one-dimensional representations (projections) of original measures that are obtained by Radon Transform (RT). Despite its efficiency in the number of supports, estimating the sliced Wasserstein requires a relatively large number of projections in high-dimensional settings. Therefore, for applications where the number of supports is relatively small compared with the dimension, e.g., several deep learning applications where the mini-batch approaches are utilized, the complexities from matrix multiplication of Radon Transform become the main computational bottleneck. To address this issue, we propose to derive projections by linearly and randomly combining a smaller number of projections which are named bottleneck projections. We explain the usage of these projections by introducing Hierarchical Radon Transform (HRT) which is constructed by applying Radon Transform variants recursively. We then formulate the approach into a new metric between measures, named Hierarchical Sliced Wasserstein (HSW) distance. By proving the injectivity of HRT, we derive the metricity of HSW. Moreover, we investigate the theoretical properties of HSW including its connection to SW variants and its computational and sample complexities. Finally, we compare the computational cost and generative quality of HSW with the conventional SW on the task of deep generative modeling using various benchmark datasets including CIFAR10, CelebA, and Tiny ImageNet.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0048BA"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="nguyen2022revisiting" class="col-sm-8"> <div class="title">Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution</div> <div class="author"> <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> </div> <div class="periodical"> <em>Neural Information Processing Systems</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2204.01188.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/UT-Austin-Data-Science-Group/CSW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:5nxA0vEk-isC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-28-4285F4?logo=googlescholar&amp;labelColor=beige" alt="28 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The conventional sliced Wasserstein is defined between two probability measures that have realizations as vectors. When comparing two probability measures over images, practitioners first need to vectorize images and then project them to one-dimensional space by using matrix multiplication between the sample matrix and the projection matrix. After that, the sliced Wasserstein is evaluated by averaging the two corresponding one-dimensional projected probability measures. However, this approach has two limitations. The first limitation is that the spatial structure of images is not captured efficiently by the vectorization step; therefore, the later slicing process becomes harder to gather the discrepancy information. The second limitation is memory inefficiency since each slicing direction is a vector that has the same dimension as the images. To address these limitations, we propose novel slicing methods for sliced Wasserstein between probability measures over images that are based on the convolution operators. We derive convolution sliced Wasserstein (CSW) and its variants via incorporating stride, dilation, and non-linear activation function into the convolution operators. We investigate the metricity of CSW as well as its sample complexity, its computational complexity, and its connection to conventional sliced Wasserstein distances. Finally, we demonstrate the favorable performance of CSW over the conventional sliced Wasserstein in comparing probability measures over images and in training deep generative modeling on images.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0048BA"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> </div> <div id="nguyen2022amortized" class="col-sm-8"> <div class="title">Amortized Projection Optimization for Sliced Wasserstein Generative Models</div> <div class="author"> <em>Khai Nguyen</em>, and <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a> </div> <div class="periodical"> <em>Neural Information Processing Systems</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2203.13417.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/UT-Austin-Data-Science-Group/AmortizedSW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:hqOjcs7Dif8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-22-4285F4?logo=googlescholar&amp;labelColor=beige" alt="22 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The conventional sliced Wasserstein is defined between two probability measures that have realizations as vectors. When comparing two probability measures over images, practitioners first need to vectorize images and then project them to one-dimensional space by using matrix multiplication between the sample matrix and the projection matrix. After that, the sliced Wasserstein is evaluated by averaging the two corresponding one-dimensional projected probability measures. However, this approach has two limitations. The first limitation is that the spatial structure of images is not captured efficiently by the vectorization step; therefore, the later slicing process becomes harder to gather the discrepancy information. The second limitation is memory inefficiency since each slicing direction is a vector that has the same dimension as the images. To address these limitations, we propose novel slicing methods for sliced Wasserstein between probability measures over images that are based on the convolution operators. We derive convolution sliced Wasserstein (CSW) and its variants via incorporating stride, dilation, and non-linear activation function into the convolution operators. We investigate the metricity of CSW as well as its sample complexity, its computational complexity, and its connection to conventional sliced Wasserstein distances. Finally, we demonstrate the favorable performance of CSW over the conventional sliced Wasserstein in comparing probability measures over images and in training deep generative modeling on images.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#4E6C50"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> </div> <div id="pmlr-v162-nguyen22e" class="col-sm-8"> <div class="title">Improving Mini-batch Optimal Transport via Partial Transportation</div> <div class="author"> <em>Khai Nguyen<sup>*</sup></em>, <a href="https://hsgser.github.io/" rel="external nofollow noopener" target="_blank">Dang Nguyen<sup>*</sup></a>, The-Anh Vu Le, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Tung Pham, Nhat Ho' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>International Conference on Machine Learning</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/UT-Austin-Data-Science-Group/Mini-batch-OT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:eQOLeE2rZwMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-48-4285F4?logo=googlescholar&amp;labelColor=beige" alt="48 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Mini-batch optimal transport (m-OT) has been widely used recently to deal with the memory issue of OT in large-scale applications. Despite their practicality, m-OT suffers from misspecified mappings, namely, mappings that are optimal on the mini-batch level but are partially wrong in the comparison with the optimal transportation plan between the original measures. Motivated by the misspecified mappings issue, we propose a novel mini-batch method by using partial optimal transport (POT) between mini-batch empirical measures, which we refer to as mini-batch partial optimal transport (m-POT). Leveraging the insight from the partial transportation, we explain the source of misspecified mappings from the m-OT and motivate why limiting the amount of transported masses among mini-batches via POT can alleviate the incorrect mappings. Finally, we carry out extensive experiments on various applications such as deep domain adaptation, partial domain adaptation, deep generative model, color transfer, and gradient flow to demonstrate the favorable performance of m-POT compared to current mini-batch methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#4E6C50"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> </div> <div id="pmlr-v162-nguyen22d" class="col-sm-8"> <div class="title">On Transportation of Mini-batches: A Hierarchical Approach</div> <div class="author"> <em>Khai Nguyen</em>, <a href="https://hsgser.github.io/" rel="external nofollow noopener" target="_blank">Dang Nguyen</a> , Quoc Nguyen, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Tung Pham, Hung Bui, Dinh Phung, Trung Le, Nhat Ho' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>International Conference on Machine Learning</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/UT-Austin-Data-Science-Group/Mini-batch-OT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:UeHWp8X0CEIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-20-4285F4?logo=googlescholar&amp;labelColor=beige" alt="20 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Mini-batch optimal transport (m-OT) has been successfully used in practical applications that involve probability measures with a very high number of supports. The m-OT solves several smaller optimal transport problems and then returns the average of their costs and transportation plans. Despite its scalability advantage, the m-OT does not consider the relationship between mini-batches which leads to undesirable estimation. Moreover, the m-OT does not approximate a proper metric between probability measures since the identity property is not satisfied. To address these problems, we propose a novel mini-batch scheme for optimal transport, named Batch of Mini-batches Optimal Transport (BoMb-OT), that finds the optimal coupling between mini-batches and it can be seen as an approximation to a well-defined distance on the space of probability measures. Furthermore, we show that the m-OT is a limit of the entropic regularized version of the BoMb-OT when the regularized parameter goes to infinity. Finally, we carry out experiments on various applications including deep generative models, deep domain adaptation, approximate Bayesian computation, color transfer, and gradient flow to show that the BoMb-OT can be widely applied and performs well in various applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#820000"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> </div> <div id="nguyen2021improving" class="col-sm-8"> <div class="title">Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein</div> <div class="author"> <em>Khai Nguyen</em> , Son Nguyen, <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Tung Pham, Hung Bui' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>International Conference on Learning Representations</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2010.01787.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-31-4285F4?logo=googlescholar&amp;labelColor=beige" alt="31 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Relational regularized autoencoder (RAE) is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the prior of latent space. A recent attempt to reduce the inner discrepancy between the prior and aggregated posterior distributions is to incorporate sliced fused Gromov-Wasserstein (SFG) between these distributions. That approach has a weakness since it treats every slicing direction similarly, meanwhile several directions are not useful for the discriminative task. To improve the discrepancy and consequently the relational regularization, we propose a new relational discrepancy, named spherical sliced fused Gromov Wasserstein (SSFG), that can find an important area of projections characterized by a von Mises-Fisher distribution. Then, we introduce two variants of SSFG to improve its performance. The first variant, named mixture spherical sliced fused Gromov Wasserstein (MSSFG), replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, named power spherical sliced fused Gromov Wasserstein (PSSFG), replaces the vMF distribution by a power spherical distribution to improve the sampling time of the vMF distribution in high dimension settings. We then apply the new discrepancies to the RAE framework to achieve its new variants. Finally, we conduct extensive experiments to show that the new autoencoders have favorable performance in learning latent manifold structure, image generation, and reconstruction.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#820000"> <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a> </abbr> <span class="award badge rounded w-100"> Spotlight </span> </div> <div id="nguyen2021distributional" class="col-sm-8"> <div class="title">Distributional Sliced-Wasserstein and Applications to Generative Modeling</div> <div class="author"> <em>Khai Nguyen</em>, <a href="https://nhatptnk8912.github.io/" rel="external nofollow noopener" target="_blank">Nhat Ho</a>, Tung Pham, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Hung Bui' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>International Conference on Learning Representations</em>, 2021 </div> <div class="periodical"> </div> <span class="honor"> Spotlight Presentation [3.78%] </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2002.07367.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/VinAIResearch/DSW" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-110-4285F4?logo=googlescholar&amp;labelColor=beige" alt="110 Google Scholar citations"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Spotlight</p> </div> <div class="abstract hidden"> <p>Sliced-Wasserstein distance (SW) and its variant, Max Sliced-Wasserstein distance (Max-SW), have been used widely in the recent years due to their fast computation and scalability even when the probability measures lie in a very high dimensional space. However, SW requires many unnecessary projection samples to approximate its value while Max-SW only uses the most important projection, which ignores the information of other useful directions. In order to account for these weaknesses, we propose a novel distance, named Distributional Sliced-Wasserstein distance (DSW), that finds an optimal distribution over projections that can balance between exploring distinctive projecting directions and the informativeness of projections themselves. We show that the DSW is a generalization of Max-SW, and it can be computed efficiently by searching for the optimal push-forward measure over a set of probability measures over the unit sphere satisfying certain regularizing constraints that favor distinct directions. Finally, we conduct extensive experiments with large-scale datasets to demonstrate the favorable performances of the proposed distances over the previous sliced-based distances in generative modeling applications.</p> </div> </div> </div> </li> </ol> </div> <h2> <a href="/publications/" style="color: inherit">Selected Preprints</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> </div> <div id="nguyen2025bayesian" class="col-sm-8"> <div class="title">Bayesian Density-Density Regression with Application to Cell-Cell Communications</div> <div class="author"> <em>Khai Nguyen</em>, Yang Ni, and <a href="https://www.ma.utexas.edu/component/cobalt/item/15-mathematics/364-mueller-peter?Itemid=1259" rel="external nofollow noopener" target="_blank">Peter Mueller</a> </div> <div class="periodical"> <em>Under Review</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2504.12617" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:q3oQSFYPqjQC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>We introduce a scalable framework for regressing multivariate distributions onto multivariate distributions, motivated by the application of inferring cell-cell communication from population-scale single-cell data. The observed data consist of pairs of multivariate distributions for ligands from one cell type and corresponding receptors from another. For each ordered pair e=(l,r) of cell types (l≠r) and each sample i=1,…,n, we observe a pair of distributions (Fei,Gei) of gene expressions for ligands and receptors of cell types l and r, respectively. The aim is to set up a regression of receptor distributions Gei given ligand distributions Fei. A key challenge is that these distributions reside in distinct spaces of differing dimensions. We formulate the regression of multivariate densities on multivariate densities using a generalized Bayes framework with the sliced Wasserstein distance between fitted and observed distributions. Finally, we use inference under such regressions to define a directed graph for cell-cell communications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> </div> <div id="nguyen2025mixture" class="col-sm-8"> <div class="title">Summarizing Bayesian Nonparametric Mixture Posterior - Sliced Optimal Transport Metrics for Gaussian Mixtures</div> <div class="author"> <em>Khai Nguyen</em>, and <a href="https://www.ma.utexas.edu/component/cobalt/item/15-mathematics/364-mueller-peter?Itemid=1259" rel="external nofollow noopener" target="_blank">Peter Mueller</a> </div> <div class="periodical"> <em>Under Review</em>, 2025 </div> <div class="periodical"> </div> <span class="honor"> Contributed Talk at BNP14, Talk in session with discussion at BAYSM25 </span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2411.14674.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:4fKUyHm3Qg0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-2-4285F4?logo=googlescholar&amp;labelColor=beige" alt="2 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> </div> <div id="nguyen2025lightspeed" class="col-sm-8"> <div class="title">Lightspeed Geometric Dataset Distance via Sliced Optimal Transport</div> <div class="author"> <em>Khai Nguyen<sup>*</sup></em> , Hai Nguyen<sup>*</sup>, Tuan Pham, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Nhat Ho' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Under Review</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2501.18901.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=im5fNaQAAAAJ&amp;citation_for_view=im5fNaQAAAAJ:B3FOqHPlNUQC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>We introduce sliced optimal transport dataset distance (s-OTDD), a model-agnostic, embedding-agnostic approach for dataset comparison that requires no training, is robust to variations in the number of classes, and can handle disjoint label sets. The core innovation is Moment Transform Projection (MTP), which maps a label, represented as a distribution over features, to a real number. Using MTP, we derive a data point projection that transforms datasets into one-dimensional distributions. The s-OTDD is defined as the expected Wasserstein distance between the projected distributions, with respect to random projection parameters. Leveraging the closed form solution of one-dimensional optimal transport, s-OTDD achieves (near-)linear computational complexity in the number of data points and feature dimensions and is independent of the number of classes. With its geometrically meaningful projection, s-OTDD strongly correlates with the optimal transport dataset distance while being more efficient than existing dataset discrepancy measures. Moreover, it correlates well with the performance gap in transfer learning and classification accuracy in data augmentation.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6B%68%61%69%6E%62@%75%74%65%78%61%73.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/khainb" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/khai-nguyen-307895155" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://www.researchgate.net/profile/Khai-Nguyen-37/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://scholar.google.com/citations?user=im5fNaQAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/khainb_ml" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <div style="display: flex; justify-content: center; align-items: center; height: 100vh;"> <div style="width: 350px; height: 350px;"> <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=LNNe9YSbldJQ3E4CCvGOl8mNJr3g6aj37ruvunxX3Zo"></script> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Khai Nguyen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/libs/jquery/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="/assets/libs/mdb/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="/assets/libs/masonry/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="/assets/libs/imagesloaded/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="/assets/libs/medium_zoom/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="/assets/libs/mathjax/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="/assets/libs/polyfill/polyfill.min.js" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZKXM8P5V7J"></script> <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-ZKXM8P5V7J');
</script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>